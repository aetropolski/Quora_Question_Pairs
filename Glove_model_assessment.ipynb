{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the GloVe model (and predicting with it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we import our packages and load all of the data for the GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model_architecture import build_model\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from build_vocab import normalize_text\n",
    "from process_and_train import text_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some NaN values in the downloaded dataset. In the code, these are dropped, but that will affect our indexing later, so I'm going to see exactly which rows have NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([105796, 201871, 363416], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data).any(1).to_numpy().nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/glove/300dim_glove_word_index.json','r') as read_file:\n",
    "    word_to_int = json.load(read_file)\n",
    "embedding_matrix = np.load('data/glove/300dim_glove_emb_matrix.npy')\n",
    "# I'm setting input_size to 248 because it will be necessary later. It can be however many words you plan on using.\n",
    "model = build_model(vocab_size = len(word_to_int) + 2, input_size = 248, embedding_dim = 300, lstm_units = 50, embedding_matrix = embedding_matrix, learning_rate = 0.001, clip_norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x17c06dfff98>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"models\\glove\\cp.ckpt\"\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can put in any two questions (or sentences) that we'd like and ask the model to predict whether they are the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_input(pair):\n",
    "    q1 = pair[0]\n",
    "    q2 = pair[1]\n",
    "    q1 = text_to_sequence(normalize_text(q1), word_to_int)\n",
    "    q2 = text_to_sequence(normalize_text(q2), word_to_int)\n",
    "    q1 = pad_sequences([q1], maxlen=248)\n",
    "    q2 = pad_sequences([q2], maxlen=248)\n",
    "    return [q1,q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that these are the same is 0.4604267477989197\n"
     ]
    }
   ],
   "source": [
    "q1 = 'When is Christmas?'\n",
    "q2 = 'What day is Christmas?'\n",
    "print('The probability that these are the same is {}'.format(model.predict(prep_for_input([q1, q2]))[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see where our model is lacking, let's see some of the decisions it made on the testing set.\n",
    "\n",
    "First we process the data so that it can be loaded back into the model. The model has been pretrained and the weights will be loaded from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_data = data.copy()\n",
    "for index, row in data.iterrows():\n",
    "    question1_words = normalize_text(row['question1'])\n",
    "    question2_words = normalize_text(row['question2'])\n",
    "    vec_data.at[index,'question1'] = text_to_sequence(question1_words, word_to_int)\n",
    "    vec_data.at[index,'question2'] = text_to_sequence(question2_words, word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec_data[['question1','question2']]\n",
    "Y = vec_data['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see which predictions correspond to which elements of the test set, we need to save the indices. As was hinted at earlier, these are a bit off because of the 3 dropped NaN rows. We'll have to deal with that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = list(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of preprocessed questions is 248\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max(vec_data['question1'].map(lambda x: len(x)).max(), vec_data['question2'].map(lambda x: len(x)).max())\n",
    "print('Maximum length of preprocessed questions is {}'.format(max_seq_length))\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']): \n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're ready to take a look at the predictions. First we save the predictions to an array, and we also round them to their nearest integer to see what the predicted class is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test['left'], X_test['right']])\n",
    "predicted_classes = np.rint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that we have almost twice as many false negatives as false positives. As we'll see below, the loss is quite high on the valuation set, so in either case we'd like to examine predictions which are wildly off (eg. probability comes out to 0.8 when the correct answer is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45656,  5584],\n",
       "       [ 9334, 20296]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80870/80870 [==============================] - 20s 243us/sample - loss: 0.4556 - acc: 0.8155\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([X_test['left'], X_test['right']], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_similarity_indices = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i][0] < 0.2:\n",
    "        low_similarity_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make adjustments for the rows that were NaN'd out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_similarity_rows = []\n",
    "for i in low_similarity_indices:\n",
    "    index = test_indices[i]\n",
    "    if index > 105796 and index < 201871:\n",
    "        index = index - 1\n",
    "    if index > 201871 and index < 363416:\n",
    "        index = index - 2\n",
    "    if index > 363416:\n",
    "        index = index - 3\n",
    "    low_similarity_rows.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see later that in addition to there being more false negatives, there are also more high confidence false negatives than false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of high confidence false negatives is 2712\n"
     ]
    }
   ],
   "source": [
    "predict_dissimilar = data.iloc[low_similarity_rows]\n",
    "false_negative = predict_dissimilar[predict_dissimilar['is_duplicate']==1]\n",
    "print('The number of high confidence false negatives is {}'.format(false_negative.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a sampling of some of the poor predictions this model made. To be honest, I find some of them to be a toss up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do you get rid of dog ticks?']\n",
      "['How do you get rid of dead ticks on dogs?']\n"
     ]
    }
   ],
   "source": [
    "# these questions are supposed to be the same but were predicted to be very dissimilar\n",
    "# i = 2378, for example\n",
    "i = np.random.choice(2712, 1)\n",
    "print(i)\n",
    "print(list(false_negative.iloc[i]['question1']))\n",
    "print(list(false_negative.iloc[i]['question2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poorly predicted false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_indices = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i][0] > 0.8:\n",
    "        high_similarity_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_rows = []\n",
    "for i in high_similarity_indices:\n",
    "    index = test_indices[i]\n",
    "    if index > 105796 and index < 201871:\n",
    "        index = index - 1\n",
    "    if index > 201871 and index < 363416:\n",
    "        index = index - 2\n",
    "    if index > 363416:\n",
    "        index = index - 3\n",
    "    high_similarity_rows.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of high confidence false positives is 1264\n"
     ]
    }
   ],
   "source": [
    "predict_similar = data.iloc[high_similarity_rows]\n",
    "false_positive = predict_similar[predict_similar['is_duplicate']==0]\n",
    "print('The number of high confidence false positives is {}'.format(false_positive.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that some of the poorly made predictions are wildly wrong, but some could go either way. It seems that the biggest problem plaguing this dataset is defining what it means for two questions to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How long do attack dogs live?']\n",
      "['How long do dogs live?']\n"
     ]
    }
   ],
   "source": [
    "# these questions are supposed to be different but were predicted to be very similar\n",
    "i = np.random.choice(1264, 1)\n",
    "print(list(false_positive.iloc[i]['question1']))\n",
    "print(list(false_positive.iloc[i]['question2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the questions 'How do you get rid of dog ticks?' and 'How do you get rid of dead ticks on dogs?' are marked as being duplicates of each other, but the questions 'How long do attack dogs live?', 'How long do dogs live?' are marked as being different. One could argue that the distinction between \"attack dogs\" and \"dogs\" is more significant than the distinction between \"dead ticks\" and \"ticks,\" but that person may not have ever had to remove a live tick from anything before. One would hope that in the case of ambiguity, the model would assign a low level of confidence (around .5), but that appears to not be the case, at least a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurately predicted positives (high confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of high confidence true positives is 10527\n"
     ]
    }
   ],
   "source": [
    "true_positive = predict_similar[predict_similar['is_duplicate']==1]\n",
    "print(\"The number of high confidence true positives is {}\".format(true_positive.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n",
      "['What will happen to the money in foreign banks after demonetizing 500 and 1000 rupee notes?']\n",
      "['How will the decision to illegalize the 500 and 1000 Rs notes help to get rid of black money in the Swiss bank or maybe in other foreign banks and currencies?']\n"
     ]
    }
   ],
   "source": [
    "# i = 947 (funny)\n",
    "# i = 104, 306 (good example)\n",
    "i = np.random.choice(1264, 1)\n",
    "print(i)\n",
    "print(list(true_positive.iloc[i]['question1']))\n",
    "print(list(true_positive.iloc[i]['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do people try to ask silly questions on Quora rather than googling it?\n",
      "Why do so many people ask things on Quora that they could just as easily Google?\n"
     ]
    }
   ],
   "source": [
    "print(true_positive.iloc[947]['question1'])\n",
    "print(true_positive.iloc[947]['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
